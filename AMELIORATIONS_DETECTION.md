# üîç Am√©liorations de la D√©tection des Messages LLM

## üìã Analyse du Syst√®me Actuel

### Fonctionnement actuel
1. **Content Script** (`content.js`) :
   - Utilise des s√©lecteurs CSS g√©n√©riques pour trouver les messages
   - Extrait le texte via `textContent`
   - Estime les tokens avec une formule approximative (chars/4)
   - Utilise des mod√®les par d√©faut (gpt-4, claude-3.5-sonnet, etc.)

### Limitations identifi√©es
1. ‚ùå **Pas d'interception des requ√™tes r√©seau** : Les vraies donn√©es (tokens, mod√®le) ne sont pas r√©cup√©r√©es
2. ‚ùå **S√©lecteurs DOM fragiles** : Les s√©lecteurs peuvent casser si les plateformes changent leur structure
3. ‚ùå **Estimation de tokens impr√©cise** : La formule chars/4 est trop approximative
4. ‚ùå **Mod√®le non d√©tect√©** : Utilise toujours le mod√®le par d√©faut, pas le vrai mod√®le utilis√©
5. ‚ùå **Messages en streaming** : Ne g√®re pas bien les messages qui se g√©n√®rent en temps r√©el
6. ‚ùå **Pas de d√©duplication** : Peut compter plusieurs fois le m√™me message

---

## üéØ Am√©liorations Propos√©es

### 1. Interception des Requ√™tes R√©seau (Priorit√© Haute)

**Objectif** : R√©cup√©rer les vraies donn√©es depuis les APIs des plateformes

#### ChatGPT (OpenAI)
- **Endpoint** : `https://chatgpt.com/backend-api/conversation` ou `/api/conversation`
- **M√©thode** : Intercepter les requ√™tes POST
- **Donn√©es disponibles** :
  - `model` : Nom exact du mod√®le (gpt-4, gpt-3.5-turbo, etc.)
  - `messages[].content` : Contenu des messages
  - `usage.prompt_tokens` : Tokens du prompt (si disponible)
  - `usage.completion_tokens` : Tokens de la r√©ponse

#### Claude (Anthropic)
- **Endpoint** : `https://api.anthropic.com/v1/messages` ou proxy via claude.ai
- **M√©thode** : Intercepter les requ√™tes POST
- **Donn√©es disponibles** :
  - `model` : Nom exact du mod√®le (claude-3-opus, claude-3.5-sonnet, etc.)
  - `input` : Contenu du prompt
  - `usage.input_tokens` : Tokens du prompt
  - `usage.output_tokens` : Tokens de la r√©ponse

#### Gemini (Google)
- **Endpoint** : `https://generativelanguage.googleapis.com/v1beta/models/*:generateContent`
- **M√©thode** : Intercepter les requ√™tes POST
- **Donn√©es disponibles** :
  - `model` : Nom du mod√®le
  - `contents` : Contenu des messages
  - `usageMetadata` : M√©tadonn√©es des tokens (si disponible)

**Impl√©mentation** :
- Utiliser `chrome.webRequest` (n√©cessite permission `webRequest`)
- Ou injecter un script qui intercepte `fetch` et `XMLHttpRequest`

### 2. Am√©lioration de la D√©tection DOM (Priorit√© Moyenne)

**Objectif** : Rendre les s√©lecteurs plus robustes et sp√©cifiques

#### ChatGPT
- **S√©lecteurs am√©lior√©s** :
  ```javascript
  messages: '[data-message-author-role], [class*="group"]',
  userMessage: '[data-message-author-role="user"], [class*="user"]',
  assistantMessage: '[data-message-author-role="assistant"], [class*="assistant"]',
  messageText: '.markdown, [class*="markdown"], .prose',
  messageContainer: '[data-testid*="conversation"], main, [role="main"]'
  ```

#### Claude
- **S√©lecteurs am√©lior√©s** :
  ```javascript
  messages: '[data-test-render-count], [class*="Message"]',
  userMessage: '[class*="UserMessage"], [data-author="user"]',
  assistantMessage: '[class*="AssistantMessage"], [data-author="assistant"]',
  messageText: '.whitespace-pre-wrap, [class*="message-text"]',
  messageContainer: '[class*="Conversation"], main'
  ```

#### Gemini
- **S√©lecteurs am√©lior√©s** :
  ```javascript
  messages: '[data-message-id], [class*="message"]',
  userMessage: '[data-author="user"], [class*="user"]',
  assistantMessage: '[data-author="model"], [class*="model"]',
  messageText: '[class*="content"], [class*="text"]',
  messageContainer: '[class*="conversation"], main'
  ```

### 3. Gestion des Messages en Streaming (Priorit√© Haute)

**Objectif** : D√©tecter et agr√©ger les messages qui se g√©n√®rent progressivement

**Strat√©gie** :
1. D√©tecter quand un message assistant commence √† √™tre g√©n√©r√©
2. Attendre que le message soit complet (plus de changements pendant 2-3 secondes)
3. Utiliser un syst√®me de "fingerprinting" pour √©viter les doublons
4. Calculer les tokens seulement une fois le message complet

**Impl√©mentation** :
```javascript
const streamingMessages = new Map(); // messageId -> {element, text, lastUpdate}

function detectStreamingMessage(element) {
  const messageId = generateMessageId(element);
  const currentText = extractMessageText(element);
  
  if (streamingMessages.has(messageId)) {
    const existing = streamingMessages.get(messageId);
    if (existing.text !== currentText) {
      // Message en cours de streaming
      existing.text = currentText;
      existing.lastUpdate = Date.now();
    } else if (Date.now() - existing.lastUpdate > 2000) {
      // Message complet (plus de changements depuis 2s)
      processCompleteMessage(element);
      streamingMessages.delete(messageId);
    }
  } else {
    streamingMessages.set(messageId, {
      element,
      text: currentText,
      lastUpdate: Date.now()
    });
  }
}
```

### 4. D√©tection du Mod√®le R√©el (Priorit√© Haute)

**Objectif** : Identifier le vrai mod√®le utilis√©, pas seulement le d√©faut

**M√©thodes** :
1. **Depuis les requ√™tes r√©seau** (meilleure m√©thode)
2. **Depuis le DOM** : Chercher dans les √©l√©ments UI (s√©lecteur de mod√®le, texte)
3. **Depuis le localStorage/sessionStorage** : Certaines plateformes stockent le mod√®le

**Impl√©mentation** :
```javascript
async function detectModel(platform) {
  // 1. Essayer depuis les requ√™tes intercept√©es
  if (interceptedModel) return interceptedModel;
  
  // 2. Essayer depuis le DOM
  const modelSelectors = {
    'chatgpt.com': '[data-model], [class*="model"], select[aria-label*="model"]',
    'claude.ai': '[data-model], [class*="model"]',
    'gemini.google.com': '[data-model], [class*="model"]'
  };
  
  const modelEl = document.querySelector(modelSelectors[platform]);
  if (modelEl) {
    const model = extractModelFromElement(modelEl);
    if (model) return model;
  }
  
  // 3. Fallback sur le mod√®le par d√©faut
  return PLATFORMS[platform].defaultModel;
}
```

### 5. Am√©lioration de l'Estimation de Tokens (Priorit√© Moyenne)

**Objectif** : Utiliser une meilleure estimation ou une biblioth√®que de tokenisation

**Options** :
1. **Formule am√©lior√©e** : Prendre en compte les mots, caract√®res, et langue
2. **Biblioth√®que tiktoken** : Utiliser tiktoken.js (plus pr√©cis mais plus lourd)
3. **API de tokenisation** : Appeler une API si disponible

**Formule am√©lior√©e** :
```javascript
function estimateTokens(text) {
  if (!text) return 0;
  
  // Approximations selon la langue
  const words = text.trim().split(/\s+/).length;
  const chars = text.length;
  
  // Pour l'anglais : ~1.3 tokens/mot, ~4 chars/token
  // Pour le fran√ßais : ~1.5 tokens/mot, ~3.5 chars/token
  const tokensFromWords = words * 1.3;
  const tokensFromChars = chars / 4;
  
  // Moyenne pond√©r√©e
  return Math.ceil((tokensFromWords * 0.6 + tokensFromChars * 0.4));
}
```

### 6. D√©duplication des Messages (Priorit√© Moyenne)

**Objectif** : √âviter de compter plusieurs fois le m√™me message

**Strat√©gie** :
- Utiliser un hash du contenu + timestamp
- Stocker les IDs des messages d√©j√† trait√©s
- V√©rifier avant de traiter un nouveau message

**Impl√©mentation** :
```javascript
const processedMessages = new Set();

function generateMessageHash(element) {
  const text = extractMessageText(element);
  const role = isUserMessage(element) ? 'user' : 'assistant';
  const timestamp = Math.floor(Date.now() / 1000); // Arrondir √† la seconde
  return `${role}-${text.substring(0, 50)}-${timestamp}`;
}

function isMessageProcessed(element) {
  const hash = generateMessageHash(element);
  if (processedMessages.has(hash)) {
    return true;
  }
  processedMessages.add(hash);
  return false;
}
```

---

## üìù Plan d'Impl√©mentation

### Phase 1 : Interception des Requ√™tes (Critique)
1. ‚úÖ Ajouter permission `webRequest` au manifest
2. ‚úÖ Cr√©er un script d'interception pour chaque plateforme
3. ‚úÖ Extraire les donn√©es (mod√®le, tokens) depuis les r√©ponses
4. ‚úÖ Stocker dans chrome.storage pour utilisation par content.js

### Phase 2 : Am√©lioration DOM (Important)
1. ‚úÖ Mettre √† jour les s√©lecteurs dans PLATFORMS
2. ‚úÖ Ajouter des m√©thodes de fallback
3. ‚úÖ Am√©liorer `extractMessageText` avec plus de s√©lecteurs

### Phase 3 : Gestion Streaming (Important)
1. ‚úÖ Impl√©menter le syst√®me de d√©tection de streaming
2. ‚úÖ Ajouter un d√©lai pour les messages complets
3. ‚úÖ Tester avec des conversations r√©elles

### Phase 4 : D√©tection Mod√®le (Important)
1. ‚úÖ Impl√©menter `detectModel()` avec toutes les m√©thodes
2. ‚úÖ Ajouter des s√©lecteurs sp√©cifiques par plateforme
3. ‚úÖ Fallback sur les mod√®les par d√©faut

### Phase 5 : Optimisations (Nice to have)
1. ‚úÖ Am√©liorer l'estimation de tokens
2. ‚úÖ Impl√©menter la d√©duplication
3. ‚úÖ Ajouter des logs pour le debugging

---

## üöÄ Prochaines √âtapes

1. **Modifier le manifest.json** : Ajouter les permissions n√©cessaires
2. **Cr√©er un intercepteur r√©seau** : Nouveau fichier `network-interceptor.js`
3. **Am√©liorer content.js** : Int√©grer toutes les am√©liorations
4. **Tester** : V√©rifier sur ChatGPT, Claude et Gemini

---

## ‚ö†Ô∏è Notes Importantes

- **Permissions** : `webRequest` n√©cessite une d√©claration dans le manifest
- **Compatibilit√©** : Certaines plateformes peuvent changer leur structure, donc il faut des fallbacks
- **Performance** : L'interception r√©seau peut ralentir l√©g√®rement, mais les donn√©es sont plus pr√©cises
- **Privacy** : Ne stocker que les m√©tadonn√©es (tokens, mod√®le), pas le contenu des messages

