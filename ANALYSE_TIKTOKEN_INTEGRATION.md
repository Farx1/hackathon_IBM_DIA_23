# üîç Analyse : Int√©gration de Tiktoken pour un Comptage de Tokens Pr√©cis

## üìã Contexte

Le projet **chatgpt-token-cost-analysis** (levysoft) utilise **tiktoken** (la librairie officielle d'OpenAI) pour calculer pr√©cis√©ment le nombre de tokens, au lieu d'estimations approximatives.

## üéØ Avantages Potentiels

### 1. **Pr√©cision du Comptage de Tokens**
- ‚úÖ **Actuellement** : Estimation bas√©e sur `chars/4`, `mots*0.75`, etc. (impr√©cise)
- ‚úÖ **Avec tiktoken** : Comptage exact selon l'encodage r√©el d'OpenAI (cl100k_base pour GPT-4, p50k_base pour GPT-3.5)
- ‚úÖ **Impact** : Calculs CO2 et √©nergie beaucoup plus pr√©cis

### 2. **S√©paration Prompt/R√©ponse**
- Le projet parse les exports JSON de ChatGPT qui ont une structure claire :
  ```json
  {
    "messages": [
      {"role": "user", "content": "..."},
      {"role": "assistant", "content": "..."}
    ]
  }
  ```
- Notre extension utilise d√©j√† `data-message-author-role` pour ChatGPT, ce qui est similaire

### 3. **Support Multi-Mod√®les**
- Tiktoken supporte diff√©rents encodages selon le mod√®le :
  - `cl100k_base` : GPT-4, GPT-3.5-turbo
  - `p50k_base` : GPT-3, Codex
  - `r50k_base` : Davinci, Curie, etc.

## üîß Options d'Int√©gration

### Option 1 : **js-tiktoken** (Recommand√©)
- üì¶ Package : `js-tiktoken` ou `@tiktoken/tokenizer`
- ‚úÖ Avantages :
  - Impl√©mentation JavaScript native
  - Compatible navigateur (peut √™tre bundl√©)
  - Support des encodages OpenAI
- ‚ö†Ô∏è Inconv√©nients :
  - Taille du bundle (~200-500 KB)
  - N√©cessite un build step

### Option 2 : **API Backend** (Alternative)
- Cr√©er un endpoint sur le serveur Express qui utilise tiktoken Python
- ‚úÖ Avantages :
  - Pas d'augmentation de taille du bundle
  - Utilise la librairie officielle Python
- ‚ö†Ô∏è Inconv√©nients :
  - N√©cessite une connexion r√©seau
  - Latence suppl√©mentaire
  - N√©cessite que le serveur soit en ligne

### Option 3 : **Hybride** (Meilleur compromis)
- Utiliser tiktoken JS pour les calculs en temps r√©el
- Garder l'estimation actuelle comme fallback si tiktoken n'est pas disponible
- ‚úÖ Avantages :
  - Pr√©cision maximale quand disponible
  - Fallback robuste
  - Pas de d√©pendance r√©seau

## üìä Comparaison Estimation vs Tiktoken

### Exemple de texte :
```
"Bonjour, comment allez-vous ? Je voudrais cr√©er une fonction Python qui calcule le nombre de tokens."
```

**Estimation actuelle** :
- Chars: 95
- Estimation: `95 / 4 = 24 tokens` (approximatif)

**Tiktoken (cl100k_base)** :
- Tokens r√©els: ~28 tokens (plus pr√©cis)

**Diff√©rence** : Pour des conversations longues, l'√©cart peut √™tre significatif (10-20% d'erreur).

## üöÄ Plan d'Impl√©mentation Recommand√©

### Phase 1 : Int√©gration de js-tiktoken
1. Installer `@tiktoken/tokenizer` ou `js-tiktoken`
2. Cr√©er un module `token-counter.js` qui :
   - D√©tecte le mod√®le utilis√©
   - Charge l'encodage appropri√©
   - Compte les tokens pr√©cis√©ment
3. Remplacer `estimateTokens()` par `countTokensWithTiktoken()` dans `content.js`

### Phase 2 : Fallback Intelligent
- Si tiktoken n'est pas disponible ‚Üí utiliser l'estimation actuelle
- Si les donn√©es intercept√©es contiennent les vrais tokens ‚Üí utiliser ceux-ci (priorit√© maximale)

### Phase 3 : Optimisation
- Lazy loading de tiktoken (charger seulement quand n√©cessaire)
- Cache des encodages pour √©viter les rechargements

## üìù Code Exemple

```javascript
// token-counter.js
import { encoding_for_model } from '@tiktoken/tokenizer';

let tokenizerCache = {};

function getTokenizerForModel(modelName) {
  if (tokenizerCache[modelName]) {
    return tokenizerCache[modelName];
  }
  
  // Mapper les noms de mod√®les aux encodages
  const modelToEncoding = {
    'gpt-4': 'cl100k_base',
    'gpt-4-turbo': 'cl100k_base',
    'gpt-3.5-turbo': 'cl100k_base',
    'gpt-3': 'p50k_base',
  };
  
  const encodingName = modelToEncoding[modelName] || 'cl100k_base';
  const tokenizer = encoding_for_model(modelName);
  tokenizerCache[modelName] = tokenizer;
  
  return tokenizer;
}

export function countTokens(text, modelName = 'gpt-4') {
  if (!text) return 0;
  
  try {
    const tokenizer = getTokenizerForModel(modelName);
    const tokens = tokenizer.encode(text);
    return tokens.length;
  } catch (error) {
    console.warn('Erreur tiktoken, fallback sur estimation:', error);
    // Fallback sur l'estimation actuelle
    return estimateTokensFallback(text);
  }
}
```

## ‚öñÔ∏è D√©cision

**Recommandation** : **Option 3 (Hybride)** avec int√©gration progressive :
1. Commencer par int√©grer js-tiktoken pour les cas o√π on a le mod√®le d√©tect√©
2. Garder l'estimation actuelle comme fallback
3. Prioriser toujours les tokens intercept√©s (les plus pr√©cis)

## üîó R√©f√©rences

- [chatgpt-token-cost-analysis](https://github.com/levysoft/chatgpt-token-cost-analysis)
- [js-tiktoken npm](https://www.npmjs.com/package/js-tiktoken)
- [@tiktoken/tokenizer npm](https://www.npmjs.com/package/@tiktoken/tokenizer)
- [OpenAI Tiktoken](https://github.com/openai/tiktoken)

